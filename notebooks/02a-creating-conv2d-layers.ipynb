{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8gPPTvI6Zg3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import torch\n",
        "from torch import nn, optim, utils\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Example Images"
      ],
      "metadata": {
        "id": "HviBIl0dZ1EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CHANNELS = 3\n",
        "\n",
        "_dataset = datasets.load_sample_images()\n",
        "_images = _dataset[\"images\"]\n",
        "\n",
        "images = (torch.stack([torch.from_numpy(arr) for arr in _images])\n",
        "               .permute(0, 3, 1, 2))"
      ],
      "metadata": {
        "id": "RbRrPHNA6eSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size, num_channels, height, width\n",
        "images.shape"
      ],
      "metadata": {
        "id": "G0BgoIF29Co4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a preprocessing function"
      ],
      "metadata": {
        "id": "EuK9Xf7UaDBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CROPPED_HEIGHT, CROPPED_WIDTH = 70, 120\n",
        "\n",
        "\n",
        "class MinMaxScaler(nn.Module):\n",
        "\n",
        "    def __init__(self, min=0, max=255) -> None:\n",
        "        super().__init__()\n",
        "        self._min = min\n",
        "        self._max = max\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        return (X - self._min) / self._max\n",
        "\n",
        "\n",
        "pre_processing_fn = nn.Sequential(\n",
        "    transforms.CenterCrop(size=(CROPPED_HEIGHT, CROPPED_WIDTH)),\n",
        "    MinMaxScaler(),\n",
        ")\n"
      ],
      "metadata": {
        "id": "huFmjMTC6vX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_images = pre_processing_fn(images)"
      ],
      "metadata": {
        "id": "jY3OnhRQ7jdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size, num_channels, cropped_height, cropped_width\n",
        "processed_images.shape"
      ],
      "metadata": {
        "id": "WcA_IKgFD0Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Use the function below to load the [UCI Handwritten Digits Dataset](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits). Create a PyTorch Dataset for this data. Create a pre-processing function for this dataset using three different random transformations. Run a batch of data through your preprocessing function to check that it works."
      ],
      "metadata": {
        "id": "UrJrbJchpAbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uci_features, uci_target = datasets.load_digits(\n",
        "    return_X_y=True,\n",
        "    as_frame=True\n",
        ")"
      ],
      "metadata": {
        "id": "_HjB_kX1pR8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uci_features.head()"
      ],
      "metadata": {
        "id": "UstYLQCtpS3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uci_target.head()"
      ],
      "metadata": {
        "id": "w6G-dIoiyI4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution:"
      ],
      "metadata": {
        "id": "UEN6_6kTaSlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_channels = 1\n",
        "input_height, input_width = 8, 8\n",
        "_image_feature_arrs = (uci_features.to_numpy()\n",
        "                           .astype(np.float32)\n",
        "                           .reshape(-1, input_channels, input_height, input_width))\n",
        "_image_feature_tensors = torch.from_numpy(_image_feature_arrs)\n",
        "\n",
        "_image_target_arr = (uci_target.to_numpy()\n",
        "                               .reshape(-1, 1))\n",
        "_image_target_tensor = torch.from_numpy(_image_target_arr)\n",
        "\n",
        "uci_dataset = utils.data.TensorDataset(\n",
        "    _image_feature_tensors,\n",
        "    _image_target_tensor\n",
        ")\n",
        "\n",
        "pre_processing_fn = nn.Sequential(\n",
        "    transforms.RandomRotation(degrees=30),\n",
        "    MinMaxScaler(),\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "X, _ = uci_dataset[:batch_size]\n",
        "Z = pre_processing_fn(X)\n",
        "print(Z.shape)"
      ],
      "metadata": {
        "id": "--j65DuIyNu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Convolutional Layer"
      ],
      "metadata": {
        "id": "ZjN8FBTMX6US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Conv2d?"
      ],
      "metadata": {
        "id": "5IsyuXZQX94F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strides and Padding"
      ],
      "metadata": {
        "id": "WyB1ENcDZcf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Valid Padding"
      ],
      "metadata": {
        "id": "FbKi2LK-qSct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_channels = 32\n",
        "kernel_height, kernel_width = 7, 7\n",
        "_stride_width, _stride_height = 1, 1\n",
        "\n",
        "conv2d_layer = nn.Conv2d(\n",
        "    in_channels=NUM_CHANNELS,\n",
        "    out_channels=out_channels,\n",
        "    kernel_size=(kernel_height, kernel_width),\n",
        "    stride=(_stride_width, _stride_height),\n",
        "    padding=0, # padding=\"valid\"\n",
        ")\n",
        "\n",
        "out_kernel_height = int((CROPPED_HEIGHT - kernel_height + _stride_height) / _stride_height)\n",
        "out_kernel_width = int((CROPPED_WIDTH - kernel_width + _stride_width) / _stride_width)"
      ],
      "metadata": {
        "id": "GseujiChXa--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = conv2d_layer(processed_images)"
      ],
      "metadata": {
        "id": "4zx-42CvYVd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size, out_channels, out_kernel_height, out_kernel_width\n",
        "output.shape"
      ],
      "metadata": {
        "id": "rSn9tcs7Yb4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_kernel_height, out_kernel_width"
      ],
      "metadata": {
        "id": "1TGryoJWlK8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Same padding"
      ],
      "metadata": {
        "id": "lL2pEx_eZnN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d_layer = nn.Conv2d(\n",
        "    in_channels=NUM_CHANNELS,\n",
        "    out_channels=out_channels,\n",
        "    kernel_size=(kernel_height, kernel_width),\n",
        "    padding=\"same\",\n",
        ")"
      ],
      "metadata": {
        "id": "3_K9pJB_Yhq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = conv2d_layer(processed_images)"
      ],
      "metadata": {
        "id": "wUBsO04fZQlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "d1dCI3jtZVib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Larger Strides"
      ],
      "metadata": {
        "id": "wo4PGSZxZp39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_stride_height, _stride_width = (2, 3)\n",
        "\n",
        "conv2d_layer = nn.Conv2d(\n",
        "    in_channels=NUM_CHANNELS,\n",
        "    out_channels=out_channels,\n",
        "    kernel_size=(kernel_height, kernel_width),\n",
        "    stride=(_stride_height, _stride_width),\n",
        ")\n",
        "\n",
        "out_kernel_height = int((CROPPED_HEIGHT - kernel_height + _stride_height) / _stride_height)\n",
        "out_kernel_width = int((CROPPED_WIDTH - kernel_width + _stride_width) / _stride_width)\n"
      ],
      "metadata": {
        "id": "e7KxG7AFZWIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = conv2d_layer(processed_images)"
      ],
      "metadata": {
        "id": "Nn0P6etDZuq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output feature maps have height and width roughly halved\n",
        "output.shape"
      ],
      "metadata": {
        "id": "f0KS1CQ6Z1bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_kernel_height, out_kernel_width"
      ],
      "metadata": {
        "id": "nXitBOiLmEaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Weight and Bias\n"
      ],
      "metadata": {
        "id": "E2oG0EFuaFQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# out_channels, in_channels, kernel_height, kernel_width\n",
        "conv2d_layer.weight.shape"
      ],
      "metadata": {
        "id": "cfETWRRbZ2SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out_channels\n",
        "conv2d_layer.bias.shape"
      ],
      "metadata": {
        "id": "I7Fnv2PeaImg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the height and width of the input images do not appear in the kernel’s shape! *All the neurons in the output feature maps share the same weights.* \n",
        "\n",
        "This means that you can feed images of any size to this layer as long as \n",
        "\n",
        "1. they are at least as large as the kernels,\n",
        "2. they have the right number of `in_channels`"
      ],
      "metadata": {
        "id": "KVggvcjRqfUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Custom Functions to Simplify Layer Creation \n",
        "\n",
        "Lots of hyperparameters to set for your convolutional layers. Often useful to create wrapper functions to enclose the values of hyperparameter settings that are common across layers."
      ],
      "metadata": {
        "id": "IuZMFTvrcP2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_conv2d_layer_(layer):\n",
        "    nn.init.xavier_normal_(layer.weight)\n",
        "    nn.init.zeros_(layer.bias)\n",
        "\n",
        "\n",
        "def create_conv2d_layer(in_channels,\n",
        "                        out_channels,\n",
        "                        kernel_height,\n",
        "                        kernel_width):\n",
        "    conv2d_layer = nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=(kernel_height, kernel_width),\n",
        "        stride=(1, 1),\n",
        "        padding=\"valid\",\n",
        "    )\n",
        "    initialize_conv2d_layer_(conv2d_layer)\n",
        "    return conv2d_layer\n",
        "\n",
        "\n",
        "def create_conv2d_block(in_channels,\n",
        "                        out_channels,\n",
        "                        kernel_height,\n",
        "                        kernel_width):\n",
        "    conv2d_layer = create_conv2d_layer(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_height,\n",
        "        kernel_width\n",
        "    )\n",
        "    conv2d_block = nn.Sequential(\n",
        "        conv2d_layer,\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    return conv2d_block\n",
        "\n"
      ],
      "metadata": {
        "id": "CvGuZnSoardB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Use the functions above to create a Conv2d block using a kernel size appropriate for your UCI Handwritten Digits images. The block should output 16 feature maps. Calculate the expected size of the output feature maps of your Conv2d block. Confirm that you calculations are correct by passing a batch of processed images through your block."
      ],
      "metadata": {
        "id": "y0aDFxjQtTne"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ly8_LCjtuG62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution:"
      ],
      "metadata": {
        "id": "DOgJRG57ZqGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels = 1\n",
        "out_channels = 16\n",
        "\n",
        "# create a convolutional backbone\n",
        "kernel_height, kernel_width = 2, 2\n",
        "conv2d_backbone = create_conv2d_block(\n",
        "    in_channels,\n",
        "    out_channels,\n",
        "    kernel_height,\n",
        "    kernel_width,\n",
        ")\n",
        "\n",
        "# calculate the output kernel size (strides are 1)\n",
        "out_kernel_height = int((input_height - kernel_height + 1) / 1)\n",
        "out_kernel_width = int((input_width - kernel_width + 1) / 1)\n",
        "\n",
        "print(out_kernel_height, out_kernel_width)\n",
        "\n",
        "output = conv2d_backbone(Z)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "4gEGzfINu150"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "The functions in the cell below will help you to create and properly initialize a classifer for your UCI Handwritten Digits dataset. Using these functions, create a classifier that takes the output of your Conv2d block and computes the probabilities that each image is in each class."
      ],
      "metadata": {
        "id": "dg9L8c0zuGZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_bias_(layer, class_probs):\n",
        "    layer.bias = nn.Parameter(class_probs)\n",
        "\n",
        "\n",
        "def create_classifier(in_channels,\n",
        "                      num_classes,\n",
        "                      class_frequencies,\n",
        "                      kernel_height,\n",
        "                      kernel_width):\n",
        "    in_features = in_channels * kernel_height * kernel_width\n",
        "    linear_layer = nn.Linear(\n",
        "        in_features,\n",
        "        num_classes,\n",
        "    )\n",
        "    initialize_bias_(linear_layer, class_frequencies)\n",
        "    return nn.Sequential(\n",
        "        linear_layer,\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "kJoLUrE_e6Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fQFeHpK9ur0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution:"
      ],
      "metadata": {
        "id": "8Vf5uxzcZe3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "class_frequencies = torch.ones(num_classes) / num_classes\n",
        "\n",
        "# create a classifier\n",
        "classifier_fn = create_classifier(\n",
        "    out_channels,\n",
        "    num_classes,\n",
        "    class_frequencies,\n",
        "    out_kernel_height,\n",
        "    out_kernel_width\n",
        ")"
      ],
      "metadata": {
        "id": "t_9BNHjhwwa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Combine your preprocessing function, your Conv2d backbone, and your classifier function to create a single sequential network that takes raw image tensors and outputs class probabilites."
      ],
      "metadata": {
        "id": "3Tr0Xu6QefzR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8izR9Ux5aQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution:"
      ],
      "metadata": {
        "id": "3z7JcJS5Zg7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine everything to get your model\n",
        "model_fn = nn.Sequential(\n",
        "    pre_processing_fn,\n",
        "    conv2d_backbone,\n",
        "    nn.Flatten(),\n",
        "    classifier_fn,\n",
        ")\n",
        "\n",
        "# should accept raw image tensors and return probs\n",
        "predicted_probas = model_fn(X)\n",
        "\n",
        "# should have shape batch_size, num_classes\n",
        "predicted_probas.shape"
      ],
      "metadata": {
        "id": "GuMYlIfecU32"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}